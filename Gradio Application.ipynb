{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "from IPython.display import Image,display, HTML\n",
    "from PIL import Image\n",
    "import base64\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "import json\n",
    "import torch\n",
    "device = torch.device(\"cpu\")  # Force CPU usage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Read environment variables\n",
    "hf_token = os.getenv(\"HF_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Correct model name\n",
    "model_name = \"sshleifer/distilbart-cnn-12-6\"\n",
    "\n",
    "# Load tokenizer & model with authentication\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, token=hf_token).to(device)\n",
    "\n",
    "# Initialize pipeline correctly\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if device.type != \"cpu\" else -1  # Use GPU/MPS if available\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(input_text: str) -> dict:\n",
    "    \"\"\"Summarizes the given text.\n",
    "\n",
    "    Args:\n",
    "        input_text (str): The input text to be summarized.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the summary.\n",
    "    \"\"\"\n",
    "    # Ensure input text is within tokenizer's max length\n",
    "    max_length = min(1024, tokenizer.model_max_length)  # Some models have higher limits\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        input_text, \n",
    "        max_length=max_length, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True\n",
    "    ).to(device)  # Move to correct device\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        num_beams=4, \n",
    "        max_length=100, \n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    # Decode output\n",
    "    summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return {\"summary\": summary_text}\n",
    "\n",
    "# Example Usage\n",
    "# print(summarize_text(\"Hugging Face provides thousands of NLP models for text summarization.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': ' These are short, famous texts in English from classic sources like the Bible or Shakespeare . Some of these texts are written in an old style of English . Not all these texts were originally written in English . But they are all well known in English today, and many express beautiful thoughts .'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\" These are short, famous texts in English from classic sources like the Bible or Shakespeare. \n",
    "Some texts have word definitions and explanations to help you. \n",
    "Some of these texts are written in an old style of English. \n",
    "Try to understand them, because the English that we speak today is based on what our great, great, great, great grandparents spoke before! \n",
    "Of course, not all these texts were originally written in English. \n",
    "The Bible, for example, is a translation. But they are all well known in English today, and many of them express beautiful thoughts.\"\"\"\n",
    "\n",
    "summarize_text(text)\n",
    "\n",
    "# print (summarize_text(text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://73aa849598240353b3.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://73aa849598240353b3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "\n",
    "# Set default port safely\n",
    "PORT = int(os.environ.get(\"PORT\", 7860))\n",
    "\n",
    "def generate_summary(input_text):\n",
    "    output = summarize_text(input_text)\n",
    "    return output\n",
    "\n",
    "gr.close_all()\n",
    "demo = gr.Interface(\n",
    "    fn=generate_summary,\n",
    "    inputs=[gr.Textbox(label=\"Text to summarize\", lines=6)],\n",
    "    outputs=[gr.Textbox(label=\"Summary\", lines=3)],\n",
    "    title=\"Text Summarization\",\n",
    "    description=\"Summarize text using a pre-trained model.\"\n",
    ")\n",
    "\n",
    "demo.launch(share=True, server_port=PORT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
